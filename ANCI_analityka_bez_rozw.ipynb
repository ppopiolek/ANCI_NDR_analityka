{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Wb6d4O-pFnIB",
        "O-FSzAbGEa_T",
        "T6BwCbqTFXUa",
        "OdD5KDAxLT5y",
        "6c3JEjqWNLJx",
        "KURzJ_XXPnCP",
        "vFEjyDnjRXX9"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Laboratorium: Systemy analizy sieciowej i wykrywania zagrożeń (NIDS/NDR)\n",
        "Ten notebook poprowadzi Cię krok po kroku przez stworzenie prototypowego silnika detekcji systemu analizy sieciowej w konwencji Proof of Concept (PoC).\n",
        "\n",
        "Dla przypomnienia, tabela:\n",
        "\n",
        "\n",
        "| **ID**   | **Kategoria**        | **Opis wymagania**                                                                                      | **Typ**        | **Proponowany sposób udowodnienia/ dodatkowe komentarze**                                                                                             |\n",
        "|----------|----------------------|---------------------------------------------------------------------------------------------------------|----------------|-----------------------------------------------------------------------------------------------------------|\n",
        "| **A.1**  | Analiza flow         | Wczytywanie plików PCAP przy użyciu NFStream.                                                            | Must-have      | -                                                   |\n",
        "| **A.2**  | Analiza flow         | Dla wczytanych przepływów wyświetlanie podsumowania statystyk flow, takich jak podsumowanie ilości przesłanych pakietów pomiędzy danymi hostami. | Must-have      | -                                                |\n",
        "| **D.1**  | Detection as a Code  | Implementacja przykładowej reguły detekcyjnej w Pythonie             | Must-have      | Napisanie przykładowej reguły i symulacja przy wykorzystaniu scapy.        |\n",
        "| **ML.1** | Machine Learning     | Klasyfikacja flow na podstawie cech, takich jak czas trwania, liczba pakietów, protokół (np. z użyciem `scikit-learn`). | Must-have      | Raport generowany przez narzędzie zawiera output z modelu, np. w postaci pewności zwróconej przez model lub wizualizacji działania modelu. |\n",
        "| **ML.2** | Machine Learning     | Redukcja liczby fałszywych pozytywów (FPR) za pomocą oceny jakości modelu i tuningu hiperparametrów.     | Must-have      | Liczenie metryk takich jak FPR, TPR lub wizualizacja macierzy konfuzji dla testowanego przypadku. |\n",
        "| **E.1**  | Enrichment           | Pobieranie podstawowych informacji o IP/domenach, np. z `geopy` lub innych źródeł Threat Intelligence przy użyciu API. | Nice-to-have      | Enrichment widoczny w raporcie generowanym przez narzędzie.                                               |\n",
        "| **V.1**  | Wizualizacja         | Mapa geograficzna przedstawiająca lokalizację adresów IP wykrytych jako podejrzane.                     | Nice-to-have   | Wizualizacja lokalizacji IP na mapie, np. przy użyciu bibliotek `folium` lub `plotly`.                     |\n"
      ],
      "metadata": {
        "id": "N5oSgT6QFKN6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##A.1. Wczytywanie plików PCAP przy użyciu NFStream\n",
        "Będziemy realizować poszczególne wymagania z tabeli zamieszczonej w instrukcji, zaczynając od A.1: Wczytywanie plików PCAP przy użyciu NFStream."
      ],
      "metadata": {
        "id": "Wb6d4O-pFnIB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Przygotowanie środowiska** - najpierw instalujemy potrzebne biblioteki, które możemy przewidzieć na start"
      ],
      "metadata": {
        "id": "JKegNBfGGzb2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yvJkyV0rFHUD"
      },
      "outputs": [],
      "source": [
        "# Instalacja wymaganych pakietów\n",
        "!pip install nfstream pandas matplotlib scapy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Krok 1:** Pobranie przykładowych plików PCAP\n",
        "\n",
        "NFStream to potężna biblioteka, która umożliwia przetwarzanie danych sieciowych na poziomie przepływów (flows). Zaczniemy od pobrania dwóch przykładowych plików PCAP do analizy - jeden zawierający normalny ruch sieciowy, drugi zawierający złośliwy ruch."
      ],
      "metadata": {
        "id": "WznHmR2jI1tK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pobranie plików z ruchem normalnym\n",
        "!wget -O normal1.pcap https://mcfp.felk.cvut.cz/publicDatasets/CTU-Normal-13/2017-07-03_capture-win2.pcap\n",
        "!wget -O normal2.pcap https://mcfp.felk.cvut.cz/publicDatasets/CTU-Normal-14/2017-07-23_capture-winFull.pcap\n",
        "!wget -O normal3.pcap https://mcfp.felk.cvut.cz/publicDatasets/CTU-Normal-24/2017-04-19_win-normal.pcap\n",
        "\n",
        "# Pobranie plików z ruchem złośliwym\n",
        "!wget -O malicious1.pcap https://mcfp.felk.cvut.cz/publicDatasets/CTU-Malware-Capture-Botnet-14/2013-10-18_capture-win15.pcap\n",
        "!wget -O malicious2.pcap https://mcfp.felk.cvut.cz/publicDatasets/CTU-Malware-Capture-Botnet-9/2013-08-20_captureWin5.pcap\n",
        "!wget -O malicious3.pcap https://mcfp.felk.cvut.cz/publicDatasets/CTU-Malware-Capture-Botnet-48/botnet-capture-20110816-sogou.pcap\n",
        "!wget -O malicious4.pcap https://mcfp.felk.cvut.cz/publicDatasets/CTU-Malware-Capture-Botnet-2/2013-08-20_capture-win2.pcap\n",
        "\n",
        "# Instalacja mergecap (jeśli jeszcze nie jest zainstalowany)\n",
        "!apt-get update && apt-get install -y wireshark-common\n",
        "\n",
        "# Połączenie plików PCAP w jeden plik normalny i jeden złośliwy\n",
        "!mergecap -w normal_traffic.pcap normal1.pcap normal2.pcap normal3.pcap\n",
        "!mergecap -w malicious_traffic.pcap malicious1.pcap malicious2.pcap malicious3.pcap malicious4.pcap\n"
      ],
      "metadata": {
        "id": "kyNdafYqIzjn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Krok 2**: Podstawowe wczytanie pliku PCAP przy użyciu NFStream\n",
        "\n",
        "Najpierw wczytamy (z systemu do środowiska Pythona) plik PCAP z normalnym ruchem i przeanalizujemy podstawowe informacje o przepływach."
      ],
      "metadata": {
        "id": "UvcbBX09KXEA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import bibliotek\n",
        "from nfstream import NFStreamer\n",
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Wczytanie pliku PCAP z normalnym ruchem przy użyciu podstawowych ustawień NFStream\n",
        "print(\"Wczytywanie pliku PCAP z normalnym ruchem (domyślne ustawienia)...\")\n",
        "normal_streamer = NFStreamer(source=\"normal_traffic.pcap\")\n",
        "normal_traffic = normal_streamer.to_pandas()\n",
        "\n",
        "# Wyświetlenie informacji o DataFrame (normalny ruch)\n",
        "print(f\"\\n✅ Wczytano {len(normal_traffic)} przepływów z normal_traffic.pcap\")\n",
        "print(\"\\nKolumny dostępne w danych (ruch normalny):\")\n",
        "print(sorted(normal_traffic.columns.tolist()))\n",
        "\n",
        "print(\"\\nPierwszych 5 przepływów (ruch normalny):\")\n",
        "display(normal_traffic.head(8))\n",
        "\n",
        "# Zapisanie listy kolumn do późniejszego porównania\n",
        "basic_columns = set(normal_traffic.columns.tolist())\n",
        "print(f\"\\nLiczba dostępnych kolumn z domyślnymi ustawieniami (normal): {len(basic_columns)}\")\n",
        "\n",
        "# --- ANALIZA LUSTARZANA DLA RUCHU ZŁOŚLIWEGO ---\n",
        "\n",
        "# Wczytanie pliku PCAP ze złośliwym ruchem\n",
        "print(\"\\nWczytywanie pliku PCAP ze złośliwym ruchem (domyślne ustawienia)...\")\n",
        "malicious_streamer = NFStreamer(source=\"malicious_traffic.pcap\")\n",
        "malicious_traffic = malicious_streamer.to_pandas()\n",
        "\n",
        "# Wyświetlenie informacji o DataFrame (ruch złośliwy)\n",
        "print(f\"\\n✅ Wczytano {len(malicious_traffic)} przepływów z malicious_traffic.pcap\")\n",
        "print(\"\\nKolumny dostępne w danych (ruch złośliwy):\")\n",
        "print(sorted(malicious_traffic.columns.tolist()))\n",
        "\n",
        "print(\"\\nPierwszych 5 przepływów (ruch złośliwy):\")\n",
        "display(malicious_traffic.head(8))\n",
        "\n",
        "# Zapisanie listy kolumn i porównanie z ruchem normalnym\n",
        "malicious_columns = set(malicious_traffic.columns.tolist())\n",
        "print(f\"\\nLiczba dostępnych kolumn z domyślnymi ustawieniami (malicious): {len(malicious_columns)}\")\n"
      ],
      "metadata": {
        "id": "MTwiSWMuJvZu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Znaczenie kolumn (atrybutów) utworzonych przez NFStream jest zgodne z listą pod tym linkiem: https://www.nfstream.org/docs/api#nflow-core-features"
      ],
      "metadata": {
        "id": "0ocNDUZyMMvh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Zadanie do wykonania 1:** Włączenie analizy statystycznej\n",
        "\n",
        "**Problem:** W podstawowej konfiguracji NFStream nie włącza domyślnie zaawansowanej analizy statystycznej przepływów. Brakuje informacji takich jak:\n",
        "\n",
        "Entropia (losowość) bajtów w przepływie\n",
        "Statystyki rozmiaru pakietów (min, max, średnia, odchylenie standardowe)\n",
        "Rozkład długości pakietów\n",
        "i wiele innych przydatnych metryk do analizy bezpieczeństwa.\n",
        "\n",
        "**Zadanie:**\n",
        "\n",
        "1. Przejrzyj dokumentację NFStream (https://nfstream.org/) i znajdź parametr, który włącza analizę statystyczną przepływów.\n",
        "2. Zmodyfikuj poniższy kod, aby wczytać ten sam plik PCAP, ale z włączoną analizą statystyczną.\n",
        "3. Porównaj kolumny dostępne w nowym DataFrame z poprzednim (basic_columns).\n",
        "4. Zidentyfikuj i wyświetl 5 nowych kolumn statystycznych, które pojawiły się w danych.\n",
        "5. Zastanów się, które z tych nowych metryk mogą być przydatne do wykrywania anomalii w sieci."
      ],
      "metadata": {
        "id": "1eZFyjoMMk5x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Zakładamy, że wcześniej istnieje już DataFrame bez analizy statystycznej i jego kolumny są zapisane w basic_columns\n",
        "\n",
        "from nfstream import NFStreamer\n",
        "import pandas as pd\n",
        "\n",
        "# Wczytanie pliku PCAP z włączoną analizą statystyczną\n",
        "print(\"Wczytywanie pliku PCAP z włączoną analizą statystyczną...\")\n",
        "\n",
        "stats_streamer = NFStreamer(\n",
        "    source=\"normal_traffic.pcap\",\n",
        "    # TODO: Włącz analizę statystyczną dodając odpowiedni parametr na podstawie dokumentacji NFStream\n",
        "    # Podpowiedź: Szukaj w dokumentacji argumentów klasy NFStreamer związanych ze statystykami\n",
        ")\n",
        "\n",
        "# Konwersja danych do pandas DataFrame\n",
        "stats_traffic = stats_streamer.to_pandas()\n",
        "print(f\"Wczytano {len(stats_traffic)} przepływów.\")\n",
        "\n",
        "# Porównanie kolumn z poprzednią wersją bez statystyk\n",
        "stats_columns = set(stats_traffic.columns.tolist())\n",
        "# TODO: Oblicz zestaw nowych kolumn, które pojawiły się po włączeniu analizy statystycznej\n",
        "# Wskazówka: użyj operatora różnicy zbiorów\n",
        "new_columns = ...\n",
        "\n",
        "# Wyświetl nowe kolumny\n",
        "print(f\"\\nLiczba nowych kolumn po włączeniu analizy statystycznej: {len(new_columns)}\")\n",
        "print(\"\\nNowe kolumny statystyczne:\")\n",
        "# TODO: Posortuj i wyświetl listę nowych kolumn\n",
        "...\n",
        "\n",
        "# TODO: Wybierz 4-5 ciekawych metryk statystycznych z nowo dodanych kolumn\n",
        "# Podpowiedź: np. minimalny, maksymalny rozmiar pakietu, odchylenie standardowe itp.\n",
        "interesting_stats = [\n",
        "    # np. 'bidirectional_min_ps', ...\n",
        "]\n",
        "\n",
        "# Wyświetlenie tych kolumn dla pierwszych 8 wierszy\n",
        "print(\"\\nWyniki dla wybranych kolumn statystycznych:\")\n",
        "display(stats_traffic[interesting_stats].head(8))\n"
      ],
      "metadata": {
        "id": "gy-S7DVYM3Ut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Zadanie do wykonania 2:** Filtrowanie ruchu sieciowego\n",
        "NFStream oferuje również możliwość filtrowania pakietów przy użyciu filtrów BPF (Berkeley Packet Filter), co jest przydatne, gdy chcemy skupić się tylko na określonym typie ruchu.\n",
        "Zadanie:\n",
        "\n",
        "1. Zmodyfikuj kod, aby:\n",
        "  - Wczytać tylko ruch TCP na portach 80 i 443 (HTTP i HTTPS)\n",
        "  - Włączyć analizę statystyczną\n",
        "\n",
        "\n",
        "1. Porównaj, ile przepływów zostało wczytanych po zastosowaniu filtra w stosunku do liczby wszystkich przepływów\n",
        "2. Wyświetl statystyki procentowe - jaki procent całego ruchu stanowi ruch HTTP/HTTPS?"
      ],
      "metadata": {
        "id": "u5HTa7GCTui1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Zadanie: Sprawdź, jaki procent całego ruchu stanowi ruch HTTP/HTTPS\n",
        "\n",
        "from nfstream import NFStreamer\n",
        "import pandas as pd\n",
        "\n",
        "# Wczytanie wszystkich przepływów z analizą statystyczną\n",
        "full_streamer = NFStreamer(\n",
        "    source=\"normal_traffic.pcap\",\n",
        "    statistical_analysis=True\n",
        ")\n",
        "all_flows = full_streamer.to_pandas()\n",
        "all_flows_count = len(all_flows)\n",
        "print(f\"Całkowita liczba przepływów: {all_flows_count}\")\n",
        "\n",
        "# Wczytanie tylko wybranego ruchu (HTTP i HTTPS)\n",
        "# TODO: Wczytaj plik jeszcze raz, ale z filtrem BPF ograniczającym ruch do portów 80 i 443\n",
        "# Podpowiedź: Sprawdź w dokumentacji NFStream, jak przekazać filtr BPF do klasy NFStreamer\n",
        "http_streamer = NFStreamer(\n",
        "    ... # Plik źródłowy\n",
        "    ... # Analiza statystyczna\n",
        "    # Tutaj wpisz parametr odpowiedzialny za filtr BPF i jego wartość\n",
        "    ...\n",
        ")\n",
        "\n",
        "http_flows = http_streamer.to_pandas()\n",
        "http_flows_count = len(http_flows)\n",
        "print(f\"Liczba przepływów HTTP/HTTPS: {http_flows_count}\")\n",
        "\n",
        "# Obliczenie procentu\n",
        "# TODO: Oblicz, jaki procent całkowitego ruchu stanowi wyfiltrowany ruch HTTP/HTTPS\n",
        "# Wskazówka: użyj dzielenia i pomnóż przez 100\n",
        "http_percentage = ...\n",
        "print(f\"Ruch HTTP/HTTPS stanowi {http_percentage:.2f}% całkowitego ruchu sieciowego\")\n"
      ],
      "metadata": {
        "id": "2-TpSr_BMMMI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Zadanie do wykonania 3:** Porównanie normalnego i złośliwego ruchu\n",
        "Mając dwa różne pliki PCAP - z normalnym i złośliwym ruchem - możemy przeprowadzić wstępne porównanie ich charakterystyk.\n",
        "\n",
        "**Zadanie:**\n",
        "\n",
        "1. Wczytaj plik z złośliwym ruchem (malicious_traffic.pcap) z włączoną analizą statystyczną\n",
        "2. Porównaj podstawowe statystyki obu plików (liczba przepływów, średnia liczba pakietów, średni rozmiar pakietów)\n",
        "3. Wybierz 3 metryki statystyczne i porównaj ich rozkłady między normalnym a złośliwym ruchem\n",
        "4. Zastanów się, które metryki mogą być najbardziej przydatne do rozróżnienia normalnego ruchu od złośliwego"
      ],
      "metadata": {
        "id": "j4mi3GL3WaB9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Wczytanie normalnego ruchu z analizą statystyczną (jeśli nie zostało wcześniej wykonane)\n",
        "if 'stats_traffic' not in locals():\n",
        "    normal_stats_streamer = NFStreamer(\n",
        "        source=\"normal_traffic.pcap\",\n",
        "        statistical_analysis=True\n",
        "    )\n",
        "    stats_traffic = normal_stats_streamer.to_pandas()\n",
        "\n",
        "# Wczytanie złośliwego ruchu\n",
        "# TODO: Wczytaj plik malicious_traffic.pcap z włączoną analizą statystyczną\n",
        "# Podpowiedź: użyj NFStreamer, tak jak wyżej, ale ze zmienionym źródłem\n",
        "malicious_streamer = ...\n",
        "\n",
        "malicious_traffic = malicious_streamer.to_pandas()\n",
        "\n",
        "# Porównanie liczby przepływów\n",
        "print(\"=== Porównanie liczby przepływów ===\")\n",
        "print(\"Normalny ruch:\", len(stats_traffic))\n",
        "print(\"Złośliwy ruch:\", len(malicious_traffic))\n",
        "\n",
        "# TODO: Oblicz średnią liczbę pakietów na przepływ w obu przypadkach\n",
        "# Wskazówka: sprawdź kolumnę, która odpowiada za liczbę pakietów w przepływie\n",
        "# Pytanie pomocnicze: Jakie mogą być różnice między normalnym a złośliwym ruchem?\n",
        "\n",
        "print(\"\\n=== Średnia liczba pakietów na przepływ ===\")\n",
        "normal_avg_packets = ...  # <- oblicz średnią\n",
        "malicious_avg_packets = ...\n",
        "print(\"Normalny ruch:\", normal_avg_packets)\n",
        "print(\"Złośliwy ruch:\", malicious_avg_packets)\n",
        "\n",
        "# TODO: Oblicz średni rozmiar pakietu (w bajtach) w obu przypadkach\n",
        "# Wskazówka: Szukaj metryki opisującej średni rozmiar pakietów (np. \"mean\")\n",
        "print(\"\\n=== Średni rozmiar pakietu ===\")\n",
        "normal_avg_ps = ...\n",
        "malicious_avg_ps = ...\n",
        "print(\"Normalny ruch:\", normal_avg_ps)\n",
        "print(\"Złośliwy ruch:\", malicious_avg_ps)\n",
        "\n",
        "# TODO: Wybierz 3 metryki statystyczne i porównaj ich rozkład dla obu typów ruchu\n",
        "# Podpowiedź: wybierz metryki, które mogą ujawniać różnice w zachowaniu (np. rozmiary pakietów, entropia)\n",
        "\n",
        "selected_columns = [\n",
        "    ...,  # <- wpisz nazwy kolumn, które chcesz porównać\n",
        "    ...,\n",
        "    ...\n",
        "]\n"
      ],
      "metadata": {
        "id": "-AsyYX4fb7Ga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A.2: Wyświetlanie podsumowania statystyk flow\n",
        "W tej części zajmiemy się analizą i wizualizacją statystyk przepływów sieciowych, w tym podsumowaniem ilości przesłanych pakietów między hostami. Ta funkcjonalność jest kluczowym elementem każdego systemu analizy sieciowej, ponieważ pozwala na szybką identyfikację wzorców komunikacji oraz potencjalnych anomalii."
      ],
      "metadata": {
        "id": "O-FSzAbGEa_T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Krok 1: Przygotowanie danych\n",
        "Najpierw załadujmy dane z obydwu plików PCAP i włączmy analizę statystyczną, aby mieć pełny dostęp do wszystkich metryk."
      ],
      "metadata": {
        "id": "WbboVRwXEq_B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Zakładamy, że mamy już zainstalowane potrzebne biblioteki i wczytane pliki PCAP\n",
        "from nfstream import NFStreamer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from IPython.display import display\n",
        "\n",
        "# Jeśli jeszcze nie wczytano danych, zróbmy to teraz\n",
        "if 'normal_traffic' not in locals() or 'malicious_traffic' not in locals():\n",
        "    print(\"Wczytywanie plików PCAP z analizą statystyczną...\")\n",
        "\n",
        "    # Wczytanie normalnego ruchu\n",
        "    normal_streamer = NFStreamer(\n",
        "        source=\"normal_traffic.pcap\",\n",
        "        statistical_analysis=True\n",
        "    )\n",
        "    normal_traffic = normal_streamer.to_pandas()\n",
        "\n",
        "    # Wczytanie złośliwego ruchu\n",
        "    malicious_streamer = NFStreamer(\n",
        "        source=\"malicious_traffic.pcap\",\n",
        "        statistical_analysis=True\n",
        "    )\n",
        "    malicious_traffic = malicious_streamer.to_pandas()\n",
        "\n",
        "    print(f\"Wczytano {len(normal_traffic)} przepływów normalnego ruchu i {len(malicious_traffic)} przepływów złośliwego ruchu.\")"
      ],
      "metadata": {
        "id": "ceGv9WzpWwYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Krok 2: Podstawowe podsumowanie statystyk przepływów\n",
        "Teraz stworzymy funkcję, która będzie generować podsumowanie statystyk flow dla dowolnego zestawu danych:"
      ],
      "metadata": {
        "id": "UpS75lMJEyA2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_flow_summary(flows_df, title=\"Podsumowanie statystyk flow\"):\n",
        "    \"\"\"\n",
        "    Generuje podstawowe podsumowanie statystyk przepływów sieciowych.\n",
        "\n",
        "    Args:\n",
        "        flows_df: DataFrame zawierający dane o przepływach\n",
        "        title: Tytuł podsumowania\n",
        "\n",
        "    Returns:\n",
        "        dict: Słownik ze statystykami\n",
        "    \"\"\"\n",
        "    if flows_df.empty:\n",
        "        print(\"Brak danych do analizy.\")\n",
        "        return {}\n",
        "\n",
        "    stats = {}\n",
        "\n",
        "    # Nagłówek podsumowania\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"{title.upper()}\")\n",
        "    print(f\"{'='*50}\")\n",
        "\n",
        "    # 1. Podstawowe statystyki\n",
        "    stats['total_flows'] = len(flows_df)\n",
        "    stats['unique_src_ips'] = flows_df['src_ip'].nunique()\n",
        "    stats['unique_dst_ips'] = flows_df['dst_ip'].nunique()\n",
        "    stats['total_packets'] = flows_df['bidirectional_packets'].sum()\n",
        "    stats['total_bytes'] = flows_df['bidirectional_bytes'].sum()\n",
        "\n",
        "    print(\"\\n1. PODSTAWOWE STATYSTYKI:\")\n",
        "    print(f\"   Liczba przepływów: {stats['total_flows']:,}\")\n",
        "    print(f\"   Liczba unikalnych adresów IP źródłowych: {stats['unique_src_ips']:,}\")\n",
        "    print(f\"   Liczba unikalnych adresów IP docelowych: {stats['unique_dst_ips']:,}\")\n",
        "    print(f\"   Łączna liczba pakietów: {stats['total_packets']:,}\")\n",
        "    print(f\"   Łączna liczba bajtów: {stats['total_bytes']:,} ({stats['total_bytes']/1024/1024:.2f} MB)\")\n",
        "\n",
        "    # 2. Statystyki protokołów\n",
        "    protocol_stats = flows_df['protocol'].value_counts()\n",
        "    stats['protocol_stats'] = protocol_stats\n",
        "\n",
        "    print(\"\\n2. STATYSTYKI PROTOKOŁÓW:\")\n",
        "    for protocol, count in protocol_stats.items():\n",
        "        percentage = 100 * count / stats['total_flows']\n",
        "        print(f\"   {protocol}: {count} przepływów ({percentage:.1f}%)\")\n",
        "\n",
        "    # 3. Statystyki aplikacji\n",
        "    if 'application_name' in flows_df.columns:\n",
        "        app_stats = flows_df['application_name'].value_counts().head(10)\n",
        "        stats['app_stats'] = app_stats\n",
        "\n",
        "        print(\"\\n3. TOP 10 APLIKACJI:\")\n",
        "        for app, count in app_stats.items():\n",
        "            percentage = 100 * count / stats['total_flows']\n",
        "            print(f\"   {app}: {count} przepływów ({percentage:.1f}%)\")\n",
        "\n",
        "    # 4. Średnie wartości\n",
        "    stats['avg_packets_per_flow'] = flows_df['bidirectional_packets'].mean()\n",
        "    stats['avg_bytes_per_flow'] = flows_df['bidirectional_bytes'].mean()\n",
        "    stats['avg_duration_ms'] = flows_df['bidirectional_duration_ms'].mean()\n",
        "\n",
        "    print(\"\\n4. ŚREDNIE WARTOŚCI:\")\n",
        "    print(f\"   Średnia liczba pakietów na przepływ: {stats['avg_packets_per_flow']:.2f}\")\n",
        "    print(f\"   Średnia liczba bajtów na przepływ: {stats['avg_bytes_per_flow']:.2f} ({stats['avg_bytes_per_flow']/1024:.2f} KB)\")\n",
        "    print(f\"   Średni czas trwania przepływu: {stats['avg_duration_ms']:.2f} ms ({stats['avg_duration_ms']/1000:.2f} s)\")\n",
        "\n",
        "    return stats\n",
        "\n",
        "# Wygenerujmy podsumowanie dla normalnego ruchu\n",
        "normal_stats = generate_flow_summary(normal_traffic, \"Podsumowanie statystyk normalnego ruchu\")"
      ],
      "metadata": {
        "id": "heUyS_KMEVVe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Krok 3: Analiza komunikacji między hostami\n",
        "Jednym z kluczowych aspektów analizy ruchu sieciowego jest zrozumienie wzorców komunikacji między hostami. Stwórzmy funkcję, która analizuje przepływy pod kątem ilości przesłanych pakietów pomiędzy parami hostów:"
      ],
      "metadata": {
        "id": "E8Hi1PNgE6xt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "6ONNB8ReE0pD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_host_communication(flows_df, top_n=10):\n",
        "    \"\"\"\n",
        "    Analizuje komunikację między hostami na podstawie ilości przesłanych pakietów i bajtów.\n",
        "\n",
        "    Args:\n",
        "        flows_df: DataFrame zawierający dane o przepływach\n",
        "        top_n: Liczba topowych par do wyświetlenia\n",
        "\n",
        "    Returns:\n",
        "        dict: Słownik z wynikami analizy\n",
        "    \"\"\"\n",
        "    if flows_df.empty:\n",
        "        print(\"Brak danych do analizy.\")\n",
        "        return {}\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"ANALIZA KOMUNIKACJI MIĘDZY HOSTAMI\")\n",
        "    print(f\"{'='*50}\")\n",
        "\n",
        "    # 1. Agregacja przepływów według par src_ip -> dst_ip\n",
        "    host_pairs = flows_df.groupby(['src_ip', 'dst_ip']).agg({\n",
        "        'bidirectional_packets': 'sum',\n",
        "        'bidirectional_bytes': 'sum',\n",
        "        'id': 'count'  # liczba przepływów\n",
        "    }).reset_index()\n",
        "\n",
        "    # Zmiana nazw kolumn dla czytelności\n",
        "    host_pairs.columns = ['src_ip', 'dst_ip', 'packets', 'bytes', 'flows']\n",
        "\n",
        "    # Sortowanie według liczby pakietów (malejąco)\n",
        "    host_pairs_by_packets = host_pairs.sort_values('packets', ascending=False).head(top_n)\n",
        "    results['top_pairs_by_packets'] = host_pairs_by_packets\n",
        "\n",
        "    # Sortowanie według liczby bajtów (malejąco)\n",
        "    host_pairs_by_bytes = host_pairs.sort_values('bytes', ascending=False).head(top_n)\n",
        "    results['top_pairs_by_bytes'] = host_pairs_by_bytes\n",
        "\n",
        "    # Sortowanie według liczby przepływów (malejąco)\n",
        "    host_pairs_by_flows = host_pairs.sort_values('flows', ascending=False).head(top_n)\n",
        "    results['top_pairs_by_flows'] = host_pairs_by_flows\n",
        "\n",
        "    # Wyświetlenie wyników analizy\n",
        "    print(f\"\\n1. TOP {top_n} PAR HOSTÓW WEDŁUG LICZBY PAKIETÓW:\")\n",
        "    for i, (_, row) in enumerate(host_pairs_by_packets.iterrows(), 1):\n",
        "        print(f\"   {i}. {row['src_ip']} -> {row['dst_ip']}: {row['packets']:,} pakietów ({row['bytes']:,} bajtów w {row['flows']} przepływach)\")\n",
        "\n",
        "    print(f\"\\n2. TOP {top_n} PAR HOSTÓW WEDŁUG LICZBY BAJTÓW:\")\n",
        "    for i, (_, row) in enumerate(host_pairs_by_bytes.iterrows(), 1):\n",
        "        print(f\"   {i}. {row['src_ip']} -> {row['dst_ip']}: {row['bytes']:,} bajtów ({row['packets']:,} pakietów w {row['flows']} przepływach)\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# Analiza komunikacji między hostami dla normalnego ruchu\n",
        "normal_comm = analyze_host_communication(normal_traffic)"
      ],
      "metadata": {
        "id": "FZvV4yPtE3yu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Krok 4: Wizualizacja statystyk flow\n",
        "Wizualizacje są niezwykle pomocne w szybkiej interpretacji danych. Stwórzmy kilka wykresów, które pomogą lepiej zrozumieć przepływy sieciowe:"
      ],
      "metadata": {
        "id": "K0LDXtaFE-7V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_flow_statistics(flows_df, title=\"Wizualizacja statystyk przepływów\"):\n",
        "    \"\"\"\n",
        "    Tworzy wizualizacje statystyk przepływów.\n",
        "\n",
        "    Args:\n",
        "        flows_df: DataFrame zawierający dane o przepływach\n",
        "        title: Tytuł dla wizualizacji\n",
        "    \"\"\"\n",
        "    if flows_df.empty:\n",
        "        print(\"Brak danych do wizualizacji.\")\n",
        "        return\n",
        "\n",
        "    # Konfiguracja estetyki wykresów\n",
        "    sns.set(style=\"whitegrid\")\n",
        "    plt.figure(figsize=(15, 12))\n",
        "    plt.suptitle(title, fontsize=16)\n",
        "\n",
        "    # 1. Wykres rozkładu protokołów\n",
        "    plt.subplot(2, 2, 1)\n",
        "    protocol_counts = flows_df['protocol'].value_counts()\n",
        "    sns.barplot(x=protocol_counts.index, y=protocol_counts.values)\n",
        "    plt.title('Rozkład protokołów')\n",
        "    plt.xlabel('Protokół')\n",
        "    plt.ylabel('Liczba przepływów')\n",
        "    plt.xticks(rotation=45)\n",
        "\n",
        "    # 2. Wykres rozkładu aplikacji (jeśli dostępne)\n",
        "    if 'application_name' in flows_df.columns:\n",
        "        plt.subplot(2, 2, 2)\n",
        "        app_counts = flows_df['application_name'].value_counts().head(10)\n",
        "        sns.barplot(x=app_counts.values, y=app_counts.index)\n",
        "        plt.title('Top 10 aplikacji')\n",
        "        plt.xlabel('Liczba przepływów')\n",
        "        plt.ylabel('Aplikacja')\n",
        "\n",
        "    # 3. Histogram rozmiaru przepływów (w bajtach)\n",
        "    plt.subplot(2, 2, 3)\n",
        "    sns.histplot(flows_df['bidirectional_bytes'], bins=30, kde=True)\n",
        "    plt.title('Rozkład rozmiaru przepływów')\n",
        "    plt.xlabel('Rozmiar przepływu [bajty]')\n",
        "    plt.ylabel('Liczba przepływów')\n",
        "    plt.xscale('log')  # Skala logarytmiczna dla lepszej wizualizacji\n",
        "\n",
        "    # 4. Histogram liczby pakietów w przepływach\n",
        "    plt.subplot(2, 2, 4)\n",
        "    sns.histplot(flows_df['bidirectional_packets'], bins=30, kde=True)\n",
        "    plt.title('Rozkład liczby pakietów w przepływach')\n",
        "    plt.xlabel('Liczba pakietów')\n",
        "    plt.ylabel('Liczba przepływów')\n",
        "    plt.xscale('log')  # Skala logarytmiczna dla lepszej wizualizacji\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.subplots_adjust(top=0.92)\n",
        "    plt.show()\n",
        "\n",
        "# Wizualizacja statystyk dla normalnego ruchu\n",
        "visualize_flow_statistics(normal_traffic, \"Wizualizacja statystyk normalnego ruchu\")"
      ],
      "metadata": {
        "id": "0f08racZE_Bk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Zadanie do wykonania 1: Porównanie statystyk normalnego i złośliwego ruchu\n",
        "Mając dostęp do danych zarówno z normalnego, jak i złośliwego ruchu, możemy przeprowadzić analizę porównawczą, aby wychwycić różnice.\n",
        "\n",
        "Zadanie:\n",
        "\n",
        "1. Wygeneruj podsumowanie statystyk dla złośliwego ruchu przy użyciu funkcji generate_flow_summary\n",
        "2. Przeanalizuj komunikację między hostami w złośliwym ruchu przy użyciu funkcji analyze_host_communication\n",
        "3. Stwórz wizualizację statystyk złośliwego ruchu przy użyciu funkcji visualize_flow_statistics\n",
        "4. Porównaj wyniki dla normalnego i złośliwego ruchu i zidentyfikuj kluczowe różnice"
      ],
      "metadata": {
        "id": "pCuAq-rtFE3X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Zadanie: Porównaj statystyki normalnego i złośliwego ruchu sieciowego\n",
        "\n",
        "# Zakładamy, że dane z normalnego ruchu i obiekt normal_stats zostały wcześniej wygenerowane,\n",
        "# np. za pomocą generate_flow_summary(stats_traffic, \"Podsumowanie normalnego ruchu\")\n",
        "\n",
        "# TODO: Wygeneruj podsumowanie statystyk dla złośliwego ruchu\n",
        "# Podpowiedź: użyj funkcji generate_flow_summary z odpowiednimi argumentami\n",
        "malicious_stats = ...  # <- uzupełnij\n",
        "\n",
        "# TODO: Przeanalizuj komunikację między hostami w złośliwym ruchu\n",
        "# Wskazówka: użyj odpowiedniej funkcji (z zaproponowanych powyżej), nie potrzebujesz dodatkowych parametrów\n",
        "malicious_comm = ...  # <- uzupełnij\n",
        "\n",
        "# TODO: Zwizualizuj statystyki przepływów w złośliwym ruchu\n",
        "# Podpowiedź: funkcja przyjmuje dane + tytuł do wykresu\n",
        "visualize_flow_statistics(...)  # <- uzupełnij\n",
        "\n",
        "# Porównanie kluczowych parametrów\n",
        "comparison_metrics = [\n",
        "    'total_flows', 'unique_src_ips', 'unique_dst_ips',\n",
        "    'avg_packets_per_flow', 'avg_bytes_per_flow', 'avg_duration_ms'\n",
        "]\n",
        "\n",
        "print(\"\\n=== PORÓWNANIE NORMALNEGO I ZŁOŚLIWEGO RUCHU ===\")\n",
        "for metric in comparison_metrics:\n",
        "    normal_value = normal_stats.get(metric, 0)\n",
        "    malicious_value = malicious_stats.get(metric, 0)\n",
        "    print(f\"{metric}: Normal={normal_value:.2f}, Malicious={malicious_value:.2f}, Ratio={malicious_value/normal_value if normal_value else 0:.2f}\")\n"
      ],
      "metadata": {
        "id": "gELgNM4vjJtV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Zadanie do wykonania 2: Analiza przepływów według kategorii aplikacji\n",
        "W bardziej zaawansowanej analizie ruchu sieciowego, możemy badać przepływy pogrupowane według kategorii aplikacji, aby zrozumieć, jakie typy ruchu dominują w sieci.\n",
        "\n",
        "Zadanie:\n",
        "\n",
        "1. Stwórz funkcję, która grupuje przepływy według kategorii aplikacji (application_category_name) i generuje podsumowanie dla każdej kategorii\n",
        "2. Zaimplementuj wizualizację, która porównuje kategorie aplikacji pod względem różnych metryk (liczba przepływów, liczba pakietów, liczba bajtów)\n",
        "3. Zastosuj tę funkcję do danych normalnego i złośliwego ruchu\n",
        "4. Sformułuj wnioski dotyczące różnic w rozkładzie kategorii aplikacji między normalnym a złośliwym ruchem"
      ],
      "metadata": {
        "id": "UvjacTU9FONw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Zadanie: Porównaj ruch normalny i złośliwy według kategorii aplikacji\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "def analyze_application_categories(flows_df, title=\"Analiza kategorii aplikacji\"):\n",
        "    if flows_df.empty or 'application_category_name' not in flows_df.columns:\n",
        "        print(\"Brak danych do analizy kategorii aplikacji.\")\n",
        "        return {}\n",
        "\n",
        "    # Grupowanie przepływów według kategorii aplikacji\n",
        "    # TODO: Pogrupuj dane i policz: liczbę przepływów, sumę pakietów i bajtów\n",
        "    # Podpowiedź: użyj groupby() i agg()\n",
        "    category_stats = flows_df.groupby('application_category_name').agg({\n",
        "        'id': ...,  # <- policz liczbę przepływów w kategorii\n",
        "        'bidirectional_packets': ...,  # <- zsumuj liczbę pakietów\n",
        "        'bidirectional_bytes': ...  # <- zsumuj liczbę bajtów\n",
        "    }).reset_index()\n",
        "\n",
        "    # TODO: Zmień nazwy kolumn na bardziej czytelne: category, flows, packets, bytes\n",
        "    # Wskazówka: użyj category_stats.columns = [...]\n",
        "    ...\n",
        "\n",
        "    # TODO: Posortuj dane malejąco według liczby przepływów (użyj metody sort_values, jeśli jest taka potrzeba poszukaj dokumentacji w Google)\n",
        "    category_stats = ...\n",
        "\n",
        "    # Wyświetlenie wyników tekstowych\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"{title.upper()}\")\n",
        "    print(f\"{'='*50}\")\n",
        "\n",
        "    for _, row in category_stats.iterrows():\n",
        "        print(f\"Kategoria: {row['category']}\")\n",
        "        print(f\"   Przepływy: {row['flows']} ({100*row['flows']/flows_df.shape[0]:.1f}%)\")\n",
        "        print(f\"   Pakiety: {row['packets']:,} ({100*row['packets']/flows_df['bidirectional_packets'].sum():.1f}%)\")\n",
        "        print(f\"   Bajty: {row['bytes']:,} ({100*row['bytes']/flows_df['bidirectional_bytes'].sum():.1f}%)\")\n",
        "        print()\n",
        "\n",
        "    # Wizualizacja\n",
        "    plt.figure(figsize=(15, 10))\n",
        "\n",
        "    # Wykres liczby przepływów według kategorii\n",
        "    plt.subplot(3, 1, 1)\n",
        "    sns.barplot(x='flows', y='category', data=category_stats)\n",
        "    plt.title('Liczba przepływów według kategorii aplikacji')\n",
        "    plt.xlabel('Liczba przepływów')\n",
        "\n",
        "    # Wykres liczby pakietów według kategorii\n",
        "    plt.subplot(3, 1, 2)\n",
        "    sns.barplot(x='packets', y='category', data=category_stats)\n",
        "    plt.title('Liczba pakietów według kategorii aplikacji')\n",
        "    plt.xlabel('Liczba pakietów')\n",
        "\n",
        "    # Wykres liczby bajtów według kategorii\n",
        "    plt.subplot(3, 1, 3)\n",
        "    sns.barplot(x='bytes', y='category', data=category_stats)\n",
        "    plt.title('Liczba bajtów według kategorii aplikacji')\n",
        "    plt.xlabel('Liczba bajtów')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.suptitle(title, fontsize=16)\n",
        "    plt.subplots_adjust(top=0.92)\n",
        "    plt.show()\n",
        "\n",
        "    return category_stats\n",
        "\n",
        "\n",
        "# TODO: Uruchom funkcję dla danych z normalnego i złośliwego ruchu\n",
        "# Podpowiedź: Wystarczy podać DataFrame i tytuł\n",
        "normal_categories = analyze_application_categories(..., \"Ruch normalny\")  # <- uzupełnij\n",
        "malicious_categories = analyze_application_categories(..., \"Ruch złośliwy\")  # <- uzupełnij\n",
        "\n",
        "# (Opcjonalnie) Na podstawie wykresów porównaj, które aplikacje dominują w każdym przypadku.\n"
      ],
      "metadata": {
        "id": "mEC_4jknkgfx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## D.1: Implementacja przykładowej reguły detekcyjnej w Pythonie\n",
        "W tej części zajmiemy się implementacją prostych reguł detekcyjnych zgodnie z paradygmatem Detection as a Code. Oznacza to, że zamiast definiować statyczne reguły w języku opisu (jak np. Zeek, Snort czy Yara), będziemy używać Pythona do wykrywania potencjalnie złośliwej aktywności w ruchu sieciowym."
      ],
      "metadata": {
        "id": "T6BwCbqTFXUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Krok 1: Prosta implementacja reguły detekcyjnej\n",
        "Zaczniemy od zdefiniowania i wdrożenia prostej reguły detekcyjnej, która będzie analizować każdy przepływ sieciowy:"
      ],
      "metadata": {
        "id": "qeL9BqAtFXc_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_suspicious_flow(flow):\n",
        "    \"\"\"\n",
        "    Wykrywa podejrzane przepływy na podstawie prostych reguł.\n",
        "\n",
        "    Args:\n",
        "        flow: Wiersz DataFrame reprezentujący pojedynczy przepływ\n",
        "\n",
        "    Returns:\n",
        "        bool: True jeśli przepływ jest podejrzany, False w przeciwnym razie\n",
        "        str: Powód uznania przepływu za podejrzany (lub None)\n",
        "    \"\"\"\n",
        "    # Domyślnie zakładamy, że przepływ nie jest podejrzany\n",
        "    is_suspicious = False\n",
        "    reason = None\n",
        "\n",
        "    # Reguła 1: Duża liczba pakietów w krótkim czasie (potencjalny DoS)\n",
        "    if flow['bidirectional_packets'] > 100 and flow['bidirectional_duration_ms'] < 1000:\n",
        "        is_suspicious = True\n",
        "        reason = f\"Duża liczba pakietów ({flow['bidirectional_packets']}) w krótkim czasie ({flow['bidirectional_duration_ms']} ms)\"\n",
        "\n",
        "    # Reguła 2: Duża ilość danych wysłanych do nietypowego portu\n",
        "    elif flow['dst_port'] not in [80, 443, 22, 53] and flow['src2dst_bytes'] > 10:\n",
        "        is_suspicious = True\n",
        "        reason = f\"Duża ilość danych ({flow['src2dst_bytes']} bajtów) wysłana do nietypowego portu {flow['dst_port']}\"\n",
        "\n",
        "    # Reguła 3: Asymetryczny przepływ (dużo więcej danych wysłanych niż odebranych)\n",
        "    elif flow['src2dst_bytes'] > 0 or flow['dst2src_bytes'] > 0 or flow['src2dst_bytes'] / flow['dst2src_bytes'] > 10:\n",
        "        is_suspicious = True\n",
        "        reason = f\"Asymetryczny przepływ (stosunek wysłanych/odebranych danych: {flow['src2dst_bytes'] / flow['dst2src_bytes']:.2f})\"\n",
        "\n",
        "    return is_suspicious, reason"
      ],
      "metadata": {
        "id": "FeITz_cbJDA-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Krok 2: Zastosowanie reguły detekcyjnej do danych\n",
        "Teraz zastosujmy naszą regułę do analizy wcześniej wczytanych danych:"
      ],
      "metadata": {
        "id": "erkxQe7gJLv-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO: poprawić reguły, albo wgrać bardziej zaawansowany plik, żeby coś wykrywało"
      ],
      "metadata": {
        "id": "7rSKUcAKJbZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_suspicious_flow(flow):\n",
        "    is_suspicious = False\n",
        "    reason = \"\"\n",
        "\n",
        "    # Przykładowe reguły detekcyjne\n",
        "    if flow['bidirectional_packets'] > 1000:\n",
        "        is_suspicious = True\n",
        "        reason = \"Duża liczba pakietów\"\n",
        "\n",
        "    elif flow['bidirectional_bytes'] > 1_000_000:\n",
        "        is_suspicious = True\n",
        "        reason = \"Duży wolumen danych\"\n",
        "\n",
        "    elif flow['src2dst_bytes'] > 0 and flow['dst2src_bytes'] == 0:\n",
        "        is_suspicious = True\n",
        "        reason = \"Asymetryczny przepływ (brak danych w jednym kierunku)\"\n",
        "\n",
        "    elif flow['dst2src_bytes'] > 0:\n",
        "        ratio = flow['src2dst_bytes'] / flow['dst2src_bytes']\n",
        "        if ratio > 100:\n",
        "            is_suspicious = True\n",
        "            reason = f\"Asymetryczny przepływ (stosunek wysłanych/odebranych danych: {ratio:.2f})\"\n",
        "\n",
        "    return is_suspicious, reason\n",
        "\n",
        "\n",
        "# Analiza złośliwego ruchu\n",
        "print(\"\\nAnaliza złośliwego ruchu...\")\n",
        "malicious_suspicious = apply_detection_rule(malicious_traffic, detect_suspicious_flow)\n",
        "\n",
        "print(f\"Wykryto {len(malicious_suspicious)} podejrzanych przepływów w złośliwym ruchu.\")\n",
        "if not malicious_suspicious.empty:\n",
        "    display(malicious_suspicious.head(10))  # Pokazujemy tylko 10 pierwszych, jeśli jest ich dużo\n",
        "\n"
      ],
      "metadata": {
        "id": "-WzxALHtJFu4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Zadanie do wykonania 1: Implementacja prostej reguły detekcyjnej dla skanowania portów\n",
        "\n",
        "Zadanie:\n",
        "\n",
        "Zaimplementuj prostą regułę detekcyjną, która będzie wykrywać potencjalne skanowanie portów. Skanowanie portów zazwyczaj charakteryzuje się:\n",
        "\n",
        "- Małą liczbą pakietów na przepływ (1-3 pakiety)\n",
        "- Krótkim czasem trwania przepływu (poniżej 500 ms)\n",
        "- Małą ilością przesłanych danych (poniżej 300 bajtów)"
      ],
      "metadata": {
        "id": "O9Gavpn7JtCx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_port_scan(flow):\n",
        "    \"\"\"\n",
        "    Sprawdza, czy przepływ wygląda na skanowanie portu.\n",
        "    Zwraca True i opis, jeśli spełnia warunki.\n",
        "    \"\"\"\n",
        "    is_port_scan = False\n",
        "    reason = None\n",
        "\n",
        "    # TODO: Sprawdź, czy przepływ spełnia WSZYSTKIE trzy warunki, w razie potrzeby sprawdź nazwy odpowiednich atrybutów w dokumentacji NFStream:\n",
        "    # - liczba pakietów ≤ 3\n",
        "    # - czas trwania < 500\n",
        "    # - liczba bajtów < 300\n",
        "\n",
        "    if ...:\n",
        "        if ...:\n",
        "            if ...:\n",
        "                is_port_scan = True\n",
        "                reason = f\"Podejrzane skanowanie: {flow['...']} pakiety, \" \\\n",
        "                         f\"{flow['...']} ms, \" \\\n",
        "                         f\"{flow['...']} bajtów\"\n",
        "\n",
        "    return is_port_scan, reason\n",
        "\n",
        "\n",
        "# TODO: Zastosuj regułę do całego DataFrame ze złośliwym ruchem\n",
        "# Podpowiedź: użyj gotowej funkcji apply_detection_rule, np.:\n",
        "# apply_detection_rule(dane, funkcja_detekcyjna)\n",
        "\n",
        "port_scan_results = apply_detection_rule(malicious_traffic, detect_port_scan)\n",
        "\n",
        "# Wyniki\n",
        "print(f\"Wykryto {len(port_scan_results)} potencjalnych skanowań portów.\")\n",
        "\n",
        "# TODO: Wyświetl kilka pierwszych wykrytych przepływów\n",
        "if not port_scan_results.empty:\n",
        "    display(port_scan_results.head(10))\n"
      ],
      "metadata": {
        "id": "ftGJI1yUJtTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Zadanie do wykonania 2: Wizualizacja wyników detekcji\n",
        "Zadanie:\n",
        "Stwórz prostą wizualizację wyników detekcji, np. wykres przedstawiający liczbę wykrytych podejrzanych przepływów według ich rodzaju (powodu detekcji)."
      ],
      "metadata": {
        "id": "A-ZibsMiKD3A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def visualize_detection_results(suspicious_flows_df):\n",
        "    \"\"\"\n",
        "    Tworzy wykres liczby wykrytych podejrzanych przepływów według ich powodu.\n",
        "\n",
        "    Args:\n",
        "        suspicious_flows_df: DataFrame z wykrytymi przepływami (musi zawierać kolumnę 'reason')\n",
        "    \"\"\"\n",
        "    if suspicious_flows_df.empty:\n",
        "        print(\"Brak danych do wizualizacji.\")\n",
        "        return\n",
        "\n",
        "    # TODO: Zlicz, ile razy wystąpił każdy powód detekcji\n",
        "    # Podpowiedź: użyj value_counts() na kolumnie 'reason'\n",
        "    reason_counts = ...  # <- uzupełnij\n",
        "\n",
        "    # TODO: Utwórz wykres słupkowy z tych danych\n",
        "    # Pytanie pomocnicze: Jaka forma wykresu najlepiej pokazuje porównanie liczebności kategorii?\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    ...  # <- narysuj wykres na podstawie reason_counts (użyj metody plot)\n",
        "    # Ustawienia wykresu (nie musisz zmieniać)\n",
        "    plt.title('Liczba wykrytych podejrzanych przepływów według powodu')\n",
        "    plt.xlabel('Powód')\n",
        "    plt.ylabel('Liczba przepływów')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# TODO: Wywołaj funkcję tylko jeśli dane z detekcji są dostępne\n",
        "# Podpowiedź: sprawdź, czy zmienna istnieje i czy DataFrame nie jest pusty\n",
        "if 'malicious_suspicious' in locals() and not malicious_suspicious.empty:\n",
        "    visualize_detection_results(...)  # <- podaj odpowiedni argument\n",
        "else:\n",
        "    print(\"Nie znaleziono danych z detekcji. Upewnij się, że wykonałeś analizę wcześniej.\")\n"
      ],
      "metadata": {
        "id": "UoBTKKmBnTL9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ML.1: Klasyfikacja flow na podstawie cech przy użyciu uczenia maszynowego\n",
        "W tej części zaimplementujemy prosty model uczenia maszynowego do klasyfikacji przepływów sieciowych jako normalne lub złośliwe. Wykorzystamy algorytm Random Forest do tej klasyfikacji i ocenimy jego skuteczność za pomocą macierzy błędów."
      ],
      "metadata": {
        "id": "OdD5KDAxLT5y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Krok 1: Przygotowanie danych do uczenia maszynowego\n",
        "Najpierw musimy przygotować dane do treningu modelu, łącząc dane z normalnego i złośliwego ruchu oraz wybierając odpowiednie cechy."
      ],
      "metadata": {
        "id": "5qCDNCvBLbdr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import potrzebnych bibliotek\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Sprawdzenie, czy mamy wczytane dane\n",
        "if 'normal_traffic' not in locals() or 'malicious_traffic' not in locals():\n",
        "    print(\"Nie znaleziono wczytanych danych. Upewnij się, że wykonałeś sekcję A.1.\")\n",
        "else:\n",
        "    # Krok 1: Dodanie etykiet do danych\n",
        "    normal_traffic['label'] = 0  # 0 oznacza normalny ruch\n",
        "    malicious_traffic['label'] = 1  # 1 oznacza złośliwy ruch\n",
        "\n",
        "    # Krok 2: Połączenie obu zbiorów danych\n",
        "    combined_traffic = pd.concat([normal_traffic, malicious_traffic], ignore_index=True)\n",
        "\n",
        "    # Krok 3: Wybór cech do treningu modelu\n",
        "    selected_features = [\n",
        "        'bidirectional_packets', 'bidirectional_bytes', 'bidirectional_duration_ms',\n",
        "        'src2dst_packets', 'src2dst_bytes', 'dst2src_packets', 'dst2src_bytes',\n",
        "        'protocol'\n",
        "    ]\n",
        "\n",
        "    # Sprawdzenie, czy wszystkie wybrane cechy są dostępne w danych\n",
        "    available_features = [feature for feature in selected_features if feature in combined_traffic.columns]\n",
        "\n",
        "    if len(available_features) < 3:\n",
        "        print(\"Za mało dostępnych cech do treningu modelu. Sprawdź dane wejściowe.\")\n",
        "    else:\n",
        "        print(f\"Dostępne cechy do treningu: {available_features}\")\n",
        "\n",
        "        # Krok 4: Przygotowanie danych do treningu\n",
        "        X = combined_traffic[available_features]\n",
        "        y = combined_traffic['label']\n",
        "\n",
        "        # Obsługa wartości null - wypełniamy je zerami\n",
        "        X = X.fillna(0)\n",
        "\n",
        "        print(f\"Zbiór danych do treningu: {X.shape[0]} przykładów, {X.shape[1]} cech\")\n",
        "        print(f\"Rozkład klas: {y.value_counts().to_dict()}\")"
      ],
      "metadata": {
        "id": "i_5LmH8ZLWve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Krok 2: Trenowanie modelu Decision Tree\n",
        "Teraz wytrenujemy model Decision Tree do klasyfikacji przepływów:"
      ],
      "metadata": {
        "id": "zlaOgtlLLk6T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Kontynuacja z poprzedniego kodu (zakładamy, że X i y są już zdefiniowane)\n",
        "if 'X' in locals() and 'y' in locals():\n",
        "    # Krok 1: Podział na zbiór treningowy i testowy\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "    print(f\"Zbiór treningowy: {X_train.shape[0]} przykładów\")\n",
        "    print(f\"Zbiór testowy: {X_test.shape[0]} przykładów\")\n",
        "\n",
        "    # Krok 2: Trenowanie modelu Decision Tree\n",
        "    print(\"Trenowanie modelu Decision Tree...\")\n",
        "    dt_classifier = DecisionTreeClassifier(random_state=42, max_depth=2)\n",
        "    dt_classifier.fit(X_train, y_train)\n",
        "\n",
        "    # Krok 3: Ocena modelu na zbiorze testowym\n",
        "    y_pred = dt_classifier.predict(X_test)\n",
        "    accuracy = (y_pred == y_test).mean() * 100\n",
        "\n",
        "    print(f\"Dokładność modelu na zbiorze testowym: {accuracy:.2f}%\")\n",
        "\n",
        "    # Krok 4: Wypisanie ważności cech\n",
        "    feature_importance = pd.DataFrame({\n",
        "        'Feature': X_train.columns,\n",
        "        'Importance': dt_classifier.feature_importances_\n",
        "    }).sort_values('Importance', ascending=False)\n",
        "\n",
        "    print(\"\\nWażność cech w modelu:\")\n",
        "    display(feature_importance)\n",
        "\n",
        "    # Krok 5: Wizualizacja ważności cech\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.barplot(x='Importance', y='Feature', data=feature_importance)\n",
        "    plt.title('Ważność cech w modelu Decision Tree')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "lhcHgJPVKCqv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Krok 3: Ocena modelu - macierz konfuzji i raport klasyfikacji\n",
        "Teraz ocenimy skuteczność naszego modelu za pomocą macierzy konfuzji i raportu klasyfikacji:"
      ],
      "metadata": {
        "id": "pUQI1D8bLsw_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# Kontynuacja z poprzedniego kodu\n",
        "if 'y_pred' in locals() and 'y_test' in locals():\n",
        "    # Krok 1: Obliczenie macierzy konfuzji\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "    # Krok 2: Wizualizacja macierzy konfuzji\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=['Normalny', 'Złośliwy'],\n",
        "                yticklabels=['Normalny', 'Złośliwy'])\n",
        "    plt.title('Macierz konfuzji (Decision Tree)')\n",
        "    plt.xlabel('Przewidziana klasa')\n",
        "    plt.ylabel('Prawdziwa klasa')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Krok 3: Wypisanie raportu klasyfikacji\n",
        "    print(\"\\nRaport klasyfikacji (Decision Tree):\")\n",
        "    report = classification_report(y_test, y_pred, target_names=['Normalny', 'Złośliwy'])\n",
        "    print(report)\n",
        "\n",
        "    # Krok 4: Obliczenie i wyświetlenie wskaźników\n",
        "    if cm.shape == (2, 2):\n",
        "        tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "        accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
        "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "        f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "        print(f\"\\nDokładność (Accuracy): {accuracy:.4f}\")\n",
        "        print(f\"Precyzja (Precision): {precision:.4f}\")\n",
        "        print(f\"Czułość (Recall): {recall:.4f}\")\n",
        "        print(f\"F1 Score: {f1_score:.4f}\")\n",
        "    else:\n",
        "        print(\"\\n⚠️ Niewłaściwy rozmiar macierzy konfuzji — klasy nie są binarne?\")\n"
      ],
      "metadata": {
        "id": "6XKiDDg1Lnep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Krok 4: Klasyfikacja przepływów i wizualizacja wyników\n",
        "Na koniec użyjemy naszego modelu do klasyfikacji przepływów i zwizualizujemy wyniki:"
      ],
      "metadata": {
        "id": "Wz2h9lo-L5dN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Kontynuacja z poprzedniego kodu\n",
        "if 'dt_classifier' in locals():\n",
        "    # Krok 1: Klasyfikacja wszystkich przepływów\n",
        "    print(\"\\nKlasyfikacja wszystkich przepływów...\")\n",
        "    combined_traffic['predicted_label'] = dt_classifier.predict(X)\n",
        "\n",
        "    # Krok 2: Obliczenie prawdopodobieństw przynależności do klas\n",
        "    combined_traffic['malicious_probability'] = dt_classifier.predict_proba(X)[:, 1]\n",
        "\n",
        "    # Krok 3: Wizualizacja rozkładu prawdopodobieństw dla obu klas\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    # Histogram dla normalnego ruchu\n",
        "    plt.hist(combined_traffic[combined_traffic['label'] == 0]['malicious_probability'],\n",
        "             bins=30, alpha=0.5, label='Normalny ruch', color='blue')\n",
        "\n",
        "    # Histogram dla złośliwego ruchu\n",
        "    plt.hist(combined_traffic[combined_traffic['label'] == 1]['malicious_probability'],\n",
        "             bins=30, alpha=0.5, label='Złośliwy ruch', color='red')\n",
        "\n",
        "    plt.title('Rozkład prawdopodobieństw klasyfikacji (Decision Tree)')\n",
        "    plt.xlabel('Prawdopodobieństwo bycia złośliwym przepływem')\n",
        "    plt.ylabel('Liczba przepływów')\n",
        "    plt.legend()\n",
        "    plt.grid(alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Krok 4: Wyświetlenie przykładowych wyników klasyfikacji\n",
        "    print(\"\\nPrzykładowe wyniki klasyfikacji:\")\n",
        "    sample_results = combined_traffic[['src_ip', 'dst_ip', 'protocol',\n",
        "                                       'bidirectional_packets', 'bidirectional_bytes',\n",
        "                                       'label', 'predicted_label', 'malicious_probability']]\n",
        "\n",
        "    print(\"\\nPrzykłady prawidłowo sklasyfikowanych normalnych przepływów:\")\n",
        "    display(sample_results[(sample_results['label'] == 0) &\n",
        "                           (sample_results['predicted_label'] == 0)].head(5))\n",
        "\n",
        "    print(\"\\nPrzykłady prawidłowo sklasyfikowanych złośliwych przepływów:\")\n",
        "    display(sample_results[(sample_results['label'] == 1) &\n",
        "                           (sample_results['predicted_label'] == 1)].head(5))\n",
        "\n",
        "    print(\"\\nPrzykłady błędnie sklasyfikowanych przepływów (fałszywe pozytywy):\")\n",
        "    display(sample_results[(sample_results['label'] == 0) &\n",
        "                           (sample_results['predicted_label'] == 1)].head(5))\n",
        "\n",
        "    print(\"\\nPrzykłady błędnie sklasyfikowanych przepływów (fałszywe negatywy):\")\n",
        "    display(sample_results[(sample_results['label'] == 1) &\n",
        "                           (sample_results['predicted_label'] == 0)].head(5))\n"
      ],
      "metadata": {
        "id": "x2X-jVQNLu9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Zadanie do wykonania 1: Rozszerzenie zestawu cech\n",
        "\n",
        "Zadanie:\n",
        "Rozszerz zestaw cech używanych do treningu modelu, dodając dodatkowe metryki dostępne w danych, takie jak:\n",
        "\n",
        "- liczbę przesłanych pakietów i bajtów zarówno w jednym, jak i w obu kierunkach komunikacji,\n",
        "- czas trwania przepływu sieciowego,\n",
        "- protokół warstwy transportowej używany w komunikacji (np. TCP, UDP),\n",
        "- oraz informacje o liczbie wystąpień konkretnych flag TCP, takich jak rozpoczęcie połączenia, jego zakończenie, reset czy potwierdzenia transmisji.\n",
        "\n",
        "\n",
        "\n",
        "Następnie porównaj wyniki klasyfikacji przed i po rozszerzeniu zestawu cech."
      ],
      "metadata": {
        "id": "msmJv1eWMDHF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# TODO: Zdefiniuj rozszerzony zestaw cech\n",
        "# Podpowiedź: dodaj cechy związane z pakietami, bajtami, czasem trwania, protokołem i flagami TCP\n",
        "extended_features = [\n",
        "    ...\n",
        "]\n",
        "\n",
        "# TODO: Sprawdź, które z tych cech faktycznie istnieją w danych\n",
        "available_extended_features = [f for f in extended_features if f in combined_traffic.columns]\n",
        "\n",
        "# TODO: Przygotuj dane do modelu\n",
        "# Wskazówka: użyj .fillna(0) na zbiorze cech\n",
        "X_extended = ...\n",
        "y = ...\n",
        "\n",
        "# TODO: Podziel dane na zbiór treningowy i testowy\n",
        "X_train_ext, X_test_ext, y_train, y_test = train_test_split(...)\n",
        "\n",
        "# TODO: Zbuduj i wytrenuj model drzewa decyzyjnego\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "clf.fit(...)\n",
        "\n",
        "# TODO: Wygeneruj predykcje i policz accuracy\n",
        "y_pred = ...\n",
        "accuracy_ext = ...\n",
        "print(f\"Dokładność modelu z rozszerzonymi cechami: {accuracy_ext:.2f}%\")\n",
        "\n",
        "# TODO: Oblicz i wypisz precision, recall i F1 score\n",
        "precision_ext = ...\n",
        "recall_ext = ...\n",
        "f1_ext = ...\n",
        "print(f\"Precision: {precision_ext:.4f}\")\n",
        "print(f\"Recall:    {recall_ext:.4f}\")\n",
        "print(f\"F1-score:  {f1_ext:.4f}\")\n",
        "\n",
        "# TODO: Narysuj macierz konfuzji\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title(\"Macierz konfuzji - model z rozszerzonymi cechami\")\n",
        "plt.xlabel(\"Przewidziana klasa\")\n",
        "plt.ylabel(\"Rzeczywista klasa\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# TODO: Wyświetl i zwizualizuj najważniejsze cechy\n",
        "importance_df = pd.DataFrame({\n",
        "    'Feature': X_train_ext.columns,\n",
        "    'Importance': clf.feature_importances_\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "print(\"\\nTop 10 najważniejszych cech:\")\n",
        "display(importance_df.head(10))\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x='Importance', y='Feature', data=importance_df.head(10))\n",
        "plt.title(\"Najważniejsze cechy (Decision Tree)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "JXlZYyMQnqAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ML.2: Poprawa wyników poprzez tuning modelu\n",
        "W tej części zajmiemy się redukcją liczby fałszywych pozytywów i poprawą wyników w naszym systemie detekcji, poprzez ocenę jakości modelu i dostrajanie jego parametrów. Skupimy się na algorytmie drzewa decyzyjnego (Decision Tree), który jest prostszy do interpretacji i wizualizacji niż Random Forest."
      ],
      "metadata": {
        "id": "6c3JEjqWNLJx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Krok 1: Przygotowanie danych i trenowanie podstawowego drzewa decyzyjnego\n",
        "Najpierw przygotujmy dane i wytrenujmy podstawowy model drzewa decyzyjnego:"
      ],
      "metadata": {
        "id": "qNtq264JNO6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import potrzebnych bibliotek\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
        "from IPython.display import display\n",
        "\n",
        "# Sprawdzenie, czy mamy wczytane dane\n",
        "if 'normal_traffic' not in locals() or 'malicious_traffic' not in locals():\n",
        "    print(\"Nie znaleziono wczytanych danych. Upewnij się, że wykonałeś sekcję A.1.\")\n",
        "else:\n",
        "    # Przygotowanie danych (jeśli jeszcze nie zostały przygotowane)\n",
        "    if 'combined_traffic' not in locals():\n",
        "        # Dodanie etykiet do danych\n",
        "        normal_traffic['label'] = 0  # 0 oznacza normalny ruch\n",
        "        malicious_traffic['label'] = 1  # 1 oznacza złośliwy ruch\n",
        "\n",
        "        # Połączenie obu zbiorów danych\n",
        "        combined_traffic = pd.concat([normal_traffic, malicious_traffic], ignore_index=True)\n",
        "\n",
        "    # Wybór cech do treningu modelu\n",
        "    selected_features = [\n",
        "        'bidirectional_packets', 'bidirectional_bytes', 'bidirectional_duration_ms',\n",
        "        'src2dst_packets', 'src2dst_bytes', 'dst2src_packets', 'dst2src_bytes',\n",
        "        'protocol'\n",
        "    ]\n",
        "\n",
        "    # Sprawdzenie, czy wszystkie wybrane cechy są dostępne w danych\n",
        "    available_features = [feature for feature in selected_features if feature in combined_traffic.columns]\n",
        "\n",
        "    if len(available_features) < 3:\n",
        "        print(\"Za mało dostępnych cech do treningu modelu. Sprawdź dane wejściowe.\")\n",
        "    else:\n",
        "        print(f\"Dostępne cechy do treningu: {available_features}\")\n",
        "\n",
        "        # Przygotowanie danych do treningu\n",
        "        X = combined_traffic[available_features]\n",
        "        y = combined_traffic['label']\n",
        "\n",
        "        # Obsługa wartości null - wypełniamy je zerami\n",
        "        X = X.fillna(0)\n",
        "\n",
        "        # Podział na zbiór treningowy i testowy\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "        print(f\"Zbiór treningowy: {X_train.shape[0]} przykładów\")\n",
        "        print(f\"Zbiór testowy: {X_test.shape[0]} przykładów\")\n",
        "\n",
        "        # Trenowanie podstawowego drzewa decyzyjnego (bez ograniczeń)\n",
        "        dt_classifier = DecisionTreeClassifier(random_state=42, max_depth = 2)\n",
        "        dt_classifier.fit(X_train, y_train)\n",
        "\n",
        "        # Ocena modelu na zbiorze testowym\n",
        "        y_pred = dt_classifier.predict(X_test)\n",
        "        accuracy = (y_pred == y_test).mean() * 100\n",
        "\n",
        "        print(f\"Dokładność podstawowego drzewa decyzyjnego: {accuracy:.2f}%\")\n",
        "\n",
        "        # Obliczenie macierzy konfuzji\n",
        "        cm = confusion_matrix(y_test, y_pred)\n",
        "        tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "        # Obliczenie FPR (False Positive Rate)\n",
        "        fpr = fp / (fp + tn)\n",
        "\n",
        "        print(f\"Liczba fałszywych pozytywów: {fp}\")\n",
        "        print(f\"False Positive Rate (FPR): {fpr:.4f}\")\n",
        "\n",
        "        # Wizualizacja macierzy konfuzji\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                   xticklabels=['Normalny', 'Złośliwy'],\n",
        "                   yticklabels=['Normalny', 'Złośliwy'])\n",
        "        plt.title('Macierz konfuzji - Podstawowe drzewo decyzyjne')\n",
        "        plt.xlabel('Przewidziana klasa')\n",
        "        plt.ylabel('Prawdziwa klasa')\n",
        "        plt.tight_layout()\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "R75oyRyfMKfj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Krok 2: Wizualizacja podstawowego drzewa decyzyjnego\n",
        "Teraz zwizualizujmy wytrenowane drzewo decyzyjne, aby lepiej zrozumieć, jak podejmuje decyzje:"
      ],
      "metadata": {
        "id": "-w91PimjNR_V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Wizualizacja drzewa decyzyjnego\n",
        "if 'dt_classifier' in locals() and 'available_features' in locals():\n",
        "    plt.figure(figsize=(20, 10))\n",
        "    plot_tree(dt_classifier, filled=True, feature_names=available_features,\n",
        "              class_names=['Normalny', 'Złośliwy'], max_depth=3, fontsize=10)\n",
        "    plt.title('Podstawowe drzewo decyzyjne (ograniczone do głębokości 3 dla czytelności)')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Sprawdzenie głębokości drzewa\n",
        "    print(f\"Głębokość podstawowego drzewa: {dt_classifier.get_depth()}\")\n",
        "    print(f\"Liczba liści podstawowego drzewa: {dt_classifier.get_n_leaves()}\")\n",
        "else:\n",
        "    print(\"Nie znaleziono drzewa decyzyjnego. Upewnij się, że kod z kroku 1 został poprawnie wykonany.\")"
      ],
      "metadata": {
        "id": "Yw4Pg_7PNSHt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Zadanie do wykonania 1 - Eksperymentowanie z parametrami drzewa decyzyjnego\n",
        "Zadanie:\n",
        "\n",
        "Zbadaj wpływ parametru max_depth (maksymalna głębokość drzewa) na skuteczność detekcji i liczbę fałszywych pozytywów. Przetestuj kilka różnych wartości tego parametru (np. 3, 5, 10, None - gdzie None oznacza nieograniczoną głębokość), wytrenuj model dla każdej wartości i porównaj wyniki.\n",
        "\n",
        "1. Dla każdej wartości max_depth:\n",
        "\n",
        "- Wytrenuj model drzewa decyzyjnego\n",
        "- Oceń dokładność modelu na zbiorze testowym\n",
        "- Oblicz liczbę fałszywych pozytywów i False Positive Rate (FPR)\n",
        "\n",
        "2. Porównaj wyniki w formie tabeli\n",
        "3. Stwórz wykres pokazujący zależność między max_depth a FPR\n",
        "4. Wybierz optymalną wartość parametru, która zapewnia dobry kompromis między dokładnością a liczbą fałszywych pozytywów"
      ],
      "metadata": {
        "id": "_5yEh7U2PAoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# TODO: Upewnij się, że dane X_train, X_test, y_train, y_test są dostępne\n",
        "# (mogą pochodzić np. z wcześniejszego zadania)\n",
        "\n",
        "# Lista wartości parametru max_depth do przetestowania\n",
        "max_depths = ...  # wypisz gębokości w formacie tablicy [...]\n",
        "\n",
        "# Lista do zapisu wyników\n",
        "results = []\n",
        "\n",
        "# TODO: Dla każdej wartości max_depth:\n",
        "for depth in max_depths:\n",
        "    # 1. Wytrenuj model DecisionTreeClassifier z daną głębokością\n",
        "    clf = DecisionTreeClassifier(max_depth=..., random_state=42)\n",
        "    clf.fit(...)\n",
        "\n",
        "    # 2. Wygeneruj predykcje na zbiorze testowym\n",
        "    y_pred = ...\n",
        "\n",
        "    # 3. Oblicz macierz konfuzji i False Positive Rate (FPR)\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "    test_accuracy = ...\n",
        "    fpr = ...\n",
        "\n",
        "    # 4. Zapisz wyniki do listy (uzupełnij)\n",
        "    results.append({\n",
        "        'max_depth': ...,\n",
        "        'accuracy': ...,\n",
        "        'false_positives': ...,\n",
        "        'false_positive_rate': ...\n",
        "    })\n",
        "\n",
        "    # 5. Narysuj drzewo decyzyjne dla wybranych głębokości\n",
        "    if depth in [3, 5]:\n",
        "        plt.figure(figsize=(16, 8))\n",
        "        plot_tree(\n",
        "            clf,\n",
        "            feature_names=X_train.columns,\n",
        "            class_names=['Normalny', 'Złośliwy'],\n",
        "            filled=True,\n",
        "            rounded=True\n",
        "        )\n",
        "        plt.title(f'Drzewo decyzyjne (max_depth={depth})')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "# TODO: Przekonwertuj wyniki do DataFrame i wyświetl jako tabelę\n",
        "results_df = pd.DataFrame(results)\n",
        "print(\"\\nWyniki dla różnych wartości parametru max_depth:\")\n",
        "display(results_df)\n",
        "\n",
        "# TODO: Narysuj wykres słupkowy pokazujący zależność między max_depth a False Positive Rate\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(results_df['max_depth'], results_df['false_positive_rate'], color='red')\n",
        "plt.title('False Positive Rate w zależności od max_depth')\n",
        "plt.xlabel('max_depth')\n",
        "plt.ylabel('False Positive Rate (FPR)')\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "L2soK5c1ovoz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## E.1: Enrichment - Pobieranie podstawowych informacji o IP/domenach\n",
        "\n",
        "W tej części zajmiemy się wzbogacaniem (enrichment) danych o przepływach sieciowych przez dodanie podstawowych informacji o adresach IP, takich jak lokalizacja geograficzna."
      ],
      "metadata": {
        "id": "KURzJ_XXPnCP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Wzbogacanie danych o informacje geolokalizacyjne\n",
        "Zadanie:\n",
        "Wzbogać tabelę z podejrzanymi przepływami o informacje geolokalizacyjne adresów IP. Użyj prostego API do określenia kraju pochodzenia dla każdego adresu IP i dodaj te informacje do tabeli wyników."
      ],
      "metadata": {
        "id": "W6i2u2m-Qz8z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import potrzebnych bibliotek\n",
        "import pandas as pd\n",
        "import requests\n",
        "\n",
        "# Prosta funkcja do sprawdzania lokalizacji adresu IP\n",
        "def get_country(ip_address):\n",
        "    \"\"\"\n",
        "    Pobiera informację o kraju dla adresu IP.\n",
        "\n",
        "    Args:\n",
        "        ip_address: Adres IP do sprawdzenia\n",
        "\n",
        "    Returns:\n",
        "        str: Nazwa kraju lub 'Nieznany'\n",
        "    \"\"\"\n",
        "    # Pomijamy adresy prywatne\n",
        "    if ip_address.startswith(('192.168.', '10.', '172.16.', '127.')):\n",
        "        return 'Adres prywatny'\n",
        "\n",
        "    try:\n",
        "        # Używamy darmowego API\n",
        "        response = requests.get(f'http://ip-api.com/json/{ip_address}', timeout=3)\n",
        "        data = response.json()\n",
        "\n",
        "        if data.get('status') == 'success':\n",
        "            return data.get('country', 'Nieznany')\n",
        "        else:\n",
        "            return 'Nieznany'\n",
        "    except:\n",
        "        return 'Błąd API'\n",
        "\n",
        "\n",
        "print(\"Oryginalna tabela:\")\n",
        "print(stats_traffic)\n",
        "\n",
        "# Dodanie informacji o kraju dla każdego adresu IP\n",
        "stats_traffic['src_country'] = suspicious_flows['src_ip'].apply(get_country)\n",
        "stats_traffic['dst_country'] = suspicious_flows['dst_ip'].apply(get_country)\n",
        "\n",
        "print(\"\\nTabela wzbogacona o lokalizacje:\")\n",
        "stats_traffic.head()"
      ],
      "metadata": {
        "id": "giI35lDtPnWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## V.2: Wizualizacja - Mapa geograficzna lokalizacji podejrzanych adresów IP\n",
        "W tej części zajmiemy się wizualizacją lokalizacji adresów IP wykrytych jako podejrzane. Dzięki wizualizacji na mapie możemy szybko zidentyfikować, z jakich obszarów geograficznych pochodzi potencjalnie złośliwy ruch."
      ],
      "metadata": {
        "id": "vFEjyDnjRXX9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Wizualizacja lokalizacji podejrzanych IP na mapie\n",
        "Zadanie:\n",
        "Stwórz prostą mapę geograficzną pokazującą lokalizacje adresów IP wykrytych jako podejrzane. Użyj biblioteki folium do stworzenia interaktywnej mapy z markerami wskazującymi lokalizacje."
      ],
      "metadata": {
        "id": "e8m0RocfRYyX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import potrzebnych bibliotek\n",
        "import pandas as pd\n",
        "import folium\n",
        "import requests\n",
        "\n",
        "# Funkcja do pobierania lokalizacji adresu IP\n",
        "def get_ip_location(ip_address):\n",
        "    \"\"\"\n",
        "    Pobiera lokalizację (kraj, szerokość i długość geograficzną) dla adresu IP.\n",
        "\n",
        "    Args:\n",
        "        ip_address: Adres IP do sprawdzenia\n",
        "\n",
        "    Returns:\n",
        "        dict: Słownik z informacjami o lokalizacji\n",
        "    \"\"\"\n",
        "    # Pomijamy adresy prywatne\n",
        "    if ip_address.startswith(('192.168.', '10.', '172.16.', '127.')):\n",
        "        return {'country': 'Adres prywatny', 'lat': 0, 'lon': 0}\n",
        "\n",
        "    try:\n",
        "        # Używamy darmowego API\n",
        "        response = requests.get(f'http://ip-api.com/json/{ip_address}', timeout=3)\n",
        "        data = response.json()\n",
        "\n",
        "        if data.get('status') == 'success':\n",
        "            return {\n",
        "                'country': data.get('country', 'Nieznany'),\n",
        "                'lat': data.get('lat', 0),\n",
        "                'lon': data.get('lon', 0)\n",
        "            }\n",
        "        else:\n",
        "            return {'country': 'Nieznany', 'lat': 0, 'lon': 0}\n",
        "    except:\n",
        "        return {'country': 'Błąd API', 'lat': 0, 'lon': 0}\n",
        "\n",
        "# Przykładowe dane (tylko 25 dla szybkości)\n",
        "suspicious_flows = stats_traffic.head(25)\n",
        "\n",
        "# Pobieramy unikalne adresy IP\n",
        "unique_ips = pd.concat([\n",
        "    suspicious_flows['src_ip'].drop_duplicates(),\n",
        "    suspicious_flows['dst_ip'].drop_duplicates()\n",
        "]).drop_duplicates().tolist()\n",
        "\n",
        "print(f\"Znaleziono {len(unique_ips)} unikalnych adresów IP.\")\n",
        "\n",
        "# Pobieranie lokalizacji dla każdego unikalnego adresu IP\n",
        "ip_locations = {}\n",
        "for ip in unique_ips:\n",
        "    print(f\"Pobieranie lokalizacji dla IP: {ip}\")\n",
        "    ip_locations[ip] = get_ip_location(ip)\n",
        "\n",
        "# Tworzenie mapy\n",
        "m = folium.Map(location=[0, 0], zoom_start=2)\n",
        "\n",
        "# Dodanie markerów dla każdego adresu IP\n",
        "for ip, location in ip_locations.items():\n",
        "    # Pomijamy adresy bez współrzędnych\n",
        "    if location['lat'] == 0 and location['lon'] == 0:\n",
        "        continue\n",
        "\n",
        "    # Dodanie markera z informacją o adresie IP\n",
        "    folium.Marker(\n",
        "        location=[location['lat'], location['lon']],\n",
        "        popup=f\"IP: {ip}<br>Kraj: {location['country']}\",\n",
        "        icon=folium.Icon(color='red', icon='info-sign')\n",
        "    ).add_to(m)\n",
        "\n",
        "# Zapisanie mapy do pliku HTML\n",
        "map_file = 'suspicious_ip_map.html'\n",
        "m.save(map_file)\n",
        "\n",
        "print(f\"\\nMapa została zapisana do pliku: {map_file}\")\n",
        "print(\"Otwórz ten plik w przeglądarce, aby zobaczyć interaktywną mapę.\")\n",
        "\n",
        "# W Google Colab możemy wyświetlić mapę bezpośrednio\n",
        "try:\n",
        "    from google.colab import output\n",
        "    output.serve_file(map_file)\n",
        "    print(\"Mapa wyświetlana poniżej:\")\n",
        "    from IPython.display import IFrame\n",
        "    display(IFrame(src=map_file, width=800, height=600))\n",
        "except:\n",
        "    pass  # Nie jesteśmy w Colab"
      ],
      "metadata": {
        "id": "967z2sJ5Pnmf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Wyświetlenie mapy w Jupyter Notebook\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "# Zapisujemy mapę do zmiennej\n",
        "map_html = m._repr_html_()\n",
        "\n",
        "# Wyświetlamy mapę bezpośrednio\n",
        "display(HTML(map_html))"
      ],
      "metadata": {
        "id": "lvtJef4jR600"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NR7-CsnMPJY9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}